{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# WillGPT ‚Üí Qdrant Upload (Google Colab)\n\nGenerate BGE-M3 embeddings and upload to Qdrant using Colab GPU.\n\n**Supports**: ChatGPT, Claude, and Claude Projects (multi-platform)\n\n**Runtime**: GPU (T4 or better recommended)\n\n## Setup Instructions:\n1. Runtime ‚Üí Change runtime type ‚Üí GPU\n2. Run cells in order\n3. Upload your `merged_conversations.json` when prompted\n4. Update configuration in cell 2 with your Qdrant credentials"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q qdrant-client sentence-transformers tqdm FlagEmbedding"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# REQUIRED: Add your Qdrant credentials here\nQDRANT_API_KEY = \"YOUR_QDRANT_API_KEY_HERE\"  # Get from Qdrant Cloud dashboard\nQDRANT_URL = \"YOUR_QDRANT_CLUSTER_URL_HERE\"  # e.g., https://xxxxx.aws.cloud.qdrant.io:6333\nCOLLECTION_NAME = \"will-gpt\"\nBATCH_SIZE = 4\n\n# Embedding Configuration\nMODEL_NAME = \"BAAI/bge-m3\"\nEMBEDDING_MODE = \"user_focused\"  # Options: balanced, user_focused, minimal, full\n\n# Expected file: merged_conversations.json (23,592 chunks)\n#   - 11,880 ChatGPT chunks\n#   - 11,690 Claude chunks  \n#   - 22 Claude Projects chunks\n\nprint(\"‚úÖ Configuration loaded\")\nprint(f\"   Collection: {COLLECTION_NAME}\")\nprint(f\"   Embedding mode: {EMBEDDING_MODE}\")\nprint(f\"   Batch size: {BATCH_SIZE}\")\nprint(f\"   Expected platforms: ChatGPT, Claude, Claude Projects\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Upload merged_conversations.json\n\n**Upload your `data/processed/merged_conversations.json` file using the file upload button on the left sidebar.**\n\nThis file contains **all platforms merged together**:\n- ChatGPT conversations\n- Claude conversations  \n- Claude Projects (user memory, project docs, custom instructions)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files\nimport json\n\nprint(\"üìÅ Upload merged_conversations.json:\")\nprint(\"   (Contains ChatGPT + Claude + Claude Projects)\")\nuploaded = files.upload()\n\n# Accept either filename\nfilename = None\nif 'merged_conversations.json' in uploaded:\n    filename = 'merged_conversations.json'\n    print(\"‚úÖ Merged conversations file uploaded\")\nelif 'processed_conversations.json' in uploaded:\n    filename = 'processed_conversations.json'\n    print(\"‚úÖ Processed conversations file uploaded\")\nelse:\n    # Use first uploaded file\n    filename = list(uploaded.keys())[0]\n    print(f\"‚úÖ Using uploaded file: {filename}\")\n\nprint(f\"\\nüìä File size: {len(uploaded[filename]) / (1024*1024):.1f} MB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport torch\nfrom FlagEmbedding import BGEM3FlagModel\nfrom tqdm.auto import tqdm\n\n# Load conversations\nprint(\"Loading conversations...\")\nwith open(filename, 'r') as f:\n    data = json.load(f)\n\n# Handle optimized format with deduplicated interpretations\ninterpretations_store = data.get('interpretations', {})\nchunks = data['chunks']\n\n# Restore interpretations from references and extract platform-specific data\nfor chunk in chunks:\n    if 'ai_interpretation_ref' in chunk:\n        interp_ref = chunk['ai_interpretation_ref']\n        interp = interpretations_store.get(interp_ref, {})\n        chunk['ai_interpretations'] = interp\n    \n    # Extract interpretations based on platform\n    platform = chunk.get('platform', 'unknown')\n    interp = chunk.get('ai_interpretations', {})\n    \n    if platform == 'chatgpt':\n        # ChatGPT format\n        user_context = interp.get('user_context_message_data', {})\n        chunk['about_user'] = user_context.get('about_user_message', '')\n        chunk['about_model'] = user_context.get('about_model_message', '')\n    elif platform == 'claude':\n        # Claude format\n        chunk['about_user'] = interp.get('user_model', '')\n        chunk['about_model'] = interp.get('thinking', '')\n    elif platform == 'claude-projects':\n        # Claude Projects format\n        chunk['about_user'] = f\"Project: {interp.get('parent_project', '')}\"\n        chunk['about_model'] = interp.get('content_type', '')\n    else:\n        chunk['about_user'] = ''\n        chunk['about_model'] = ''\n\nprint(f\"‚úÖ Loaded {len(chunks)} chunks\")\nif interpretations_store:\n    print(f\"   Unique interpretations: {len(interpretations_store)}\")\n\n# Count platforms\nplatforms = {}\nfor chunk in chunks:\n    platform = chunk.get('platform', 'unknown')\n    platforms[platform] = platforms.get(platform, 0) + 1\n\nprint(f\"\\nPlatform distribution:\")\nfor platform, count in sorted(platforms.items()):\n    print(f\"   {platform}: {count:,} chunks\")\n\n# Check GPU\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"\\nDevice: {device}\")\nif device == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\n# Load BGE-M3 with FlagEmbedding\nprint(f\"\\nLoading {MODEL_NAME}...\")\nuse_fp16 = device == 'cuda'\nmodel = BGEM3FlagModel(MODEL_NAME, use_fp16=use_fp16, device=device)\nvector_size = 1024  # BGE-M3 dense vector dimension\nprint(f\"‚úÖ Model loaded (vector size: {vector_size}, FP16: {use_fp16})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Embedding Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def to_embedding_text(chunk, mode=\"balanced\", max_assistant_chars=3000):\n    \"\"\"Generate embedding text from chunk - supports all platforms\"\"\"\n    parts = []\n    \n    # Topic context\n    if chunk.get('conversation_title') and chunk['conversation_title'] != \"Untitled\":\n        parts.append(f\"[TOPIC: {chunk['conversation_title']}]\")\n    \n    # Platform-specific handling\n    platform = chunk.get('platform', 'unknown')\n    \n    # Add platform marker for better search\n    if platform == 'claude-projects':\n        parts.append(f\"[CLAUDE-PROJECTS]\")\n    \n    # User message\n    if chunk.get('user_message'):\n        if mode == \"minimal\":\n            return chunk['user_message']\n        parts.append(chunk['user_message'])\n    \n    # Assistant response\n    if chunk.get('assistant_message') and mode in [\"balanced\", \"full\"]:\n        assistant = chunk['assistant_message']\n        if mode == \"balanced\" and len(assistant) > max_assistant_chars:\n            half = max_assistant_chars // 2\n            assistant = assistant[:half] + \"\\n[...]\\n\" + assistant[-half:]\n        parts.append(f\"[RESPONSE] {assistant}\")\n    \n    # AI interpretations (already extracted by platform in previous cell)\n    if chunk.get('about_user'):\n        parts.append(f\"[AI_UNDERSTANDING] {chunk['about_user']}\")\n    if chunk.get('about_model'):\n        parts.append(f\"[AI_NOTES] {chunk['about_model']}\")\n    \n    return '\\n\\n'.join(parts)\n\n# Generate texts\nprint(\"Generating embedding texts for all platforms...\")\nembedding_texts = [to_embedding_text(chunk, mode=EMBEDDING_MODE) for chunk in tqdm(chunks)]\nprint(f\"‚úÖ Generated {len(embedding_texts)} embedding texts\")\n\n# Show stats by platform\nprint(f\"\\nEmbedding text stats by platform:\")\nplatforms = {}\nfor chunk in chunks:\n    platform = chunk.get('platform', 'unknown')\n    if platform not in platforms:\n        platforms[platform] = []\n    platforms[platform].append(to_embedding_text(chunk, mode=EMBEDDING_MODE))\n\nfor platform, texts in sorted(platforms.items()):\n    avg_len = sum(len(t) for t in texts) / len(texts)\n    print(f\"  {platform}: {len(texts):,} texts, avg {avg_len:.0f} chars\")\n\nprint(f\"\\nOverall avg length: {sum(len(t) for t in embedding_texts) / len(embedding_texts):.0f} chars\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Generate Embeddings (Dense + Sparse)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Generating embeddings with hybrid search (batch size: {BATCH_SIZE})...\")\nprint(f\"Mode: Dense + Sparse (lexical weights)\")\n\noutput = model.encode(\n    embedding_texts,\n    return_dense=True,\n    return_sparse=True,  # Enable sparse vectors for hybrid search\n    return_colbert_vecs=False,\n    batch_size=BATCH_SIZE,\n)\n\n# Extract dense and sparse embeddings\ndense_embeddings = output['dense_vecs']  # numpy array: [batch_size, 1024]\nsparse_embeddings = output['lexical_weights']  # list of dicts: [{'token_id': weight}]\n\nprint(f\"‚úÖ Generated {len(dense_embeddings)} dense + sparse embedding pairs\")\nprint(f\"Dense shape: {dense_embeddings.shape}\")\nprint(f\"Sparse vectors: {len(sparse_embeddings)} lexical weight mappings\")\n\n# Show example sparse vector\nif sparse_embeddings and sparse_embeddings[0]:\n    example_tokens = list(sparse_embeddings[0].items())[:5]\n    print(f\"\\nExample sparse vector (first 5 tokens): {example_tokens}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Connect to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct, SparseVectorParams\n\nprint(\"Connecting to Qdrant...\")\nclient = QdrantClient(\n    url=QDRANT_URL,\n    api_key=QDRANT_API_KEY,\n    prefer_grpc=False,\n)\n\n# Check existing collections\ncollections = client.get_collections()\nprint(f\"‚úÖ Connected!\")\nprint(f\"Existing collections: {[c.name for c in collections.collections]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if collection exists\ncollection_exists = any(c.name == COLLECTION_NAME for c in collections.collections)\n\nif collection_exists:\n    print(f\"‚úÖ Collection '{COLLECTION_NAME}' already exists!\")\n    \n    # Get existing collection info\n    collection_info = client.get_collection(COLLECTION_NAME)\n    print(f\"   Current points: {collection_info.points_count:,}\")\n    print(f\"   Vectors: dense (1024 dims) + sparse\")\n    \n    delete = input(\"\\nDelete and recreate? (yes/no): \")\n    if delete.lower() == 'yes':\n        client.delete_collection(COLLECTION_NAME)\n        print(f\"‚úÖ Deleted existing collection\")\n        collection_exists = False\n    else:\n        print(\"‚úÖ Keeping existing collection (will update/add points)\")\n\nif not collection_exists:\n    print(f\"\\nCreating collection '{COLLECTION_NAME}' with hybrid search...\")\n    from qdrant_client.models import SparseVectorParams\n    \n    client.create_collection(\n        collection_name=COLLECTION_NAME,\n        vectors_config={\n            \"dense\": VectorParams(\n                size=vector_size,\n                distance=Distance.COSINE,\n            )\n        },\n        sparse_vectors_config={\n            \"sparse\": SparseVectorParams()\n        }\n    )\n    print(f\"‚úÖ Collection created with dense + sparse vectors\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Upload to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from qdrant_client.models import SparseVector\nfrom datetime import datetime\n\nprint(f\"\\nUploading {len(dense_embeddings)} points with hybrid vectors to Qdrant...\")\nprint(f\"Platforms: ChatGPT, Claude, Claude Projects\")\n\n# Create points with both dense and sparse vectors\npoints = []\nfor idx, (chunk, dense_emb, sparse_weights) in enumerate(tqdm(zip(chunks, dense_embeddings, sparse_embeddings), total=len(chunks))):\n    \n    # Convert timestamp to UNIX timestamp (float) for Qdrant range filtering\n    timestamp_str = chunk.get('timestamp')\n    timestamp_float = None\n    if timestamp_str:\n        try:\n            # Parse ISO format timestamp and convert to UNIX timestamp\n            dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))\n            timestamp_float = dt.timestamp()\n        except Exception as e:\n            print(f\"Warning: Could not parse timestamp '{timestamp_str}': {e}\")\n    \n    # Build payload with fields common to all platforms\n    payload = {\n        \"conversation_id\": chunk.get('conversation_id'),\n        \"platform\": chunk.get('platform'),\n        \"timestamp\": timestamp_float,  # FLOAT for range filtering\n        \"timestamp_iso\": timestamp_str,  # Keep ISO string for display\n        \"conversation_title\": chunk.get('conversation_title'),\n        \"turn_number\": chunk.get('turn_number', 0),\n        \"user_message\": chunk.get('user_message'),\n        \"assistant_message\": chunk.get('assistant_message'),\n        \"assistant_model\": chunk.get('assistant_model'),\n        \"user_message_type\": chunk.get('user_message_type'),\n        \"assistant_message_type\": chunk.get('assistant_message_type'),\n    }\n    \n    # Add optional fields if present\n    if chunk.get('system_context'):\n        payload['system_context'] = chunk['system_context']\n    \n    if chunk.get('tool_usage'):\n        payload['tool_usage'] = chunk['tool_usage']\n        payload['has_tool_usage'] = True\n    else:\n        payload['has_tool_usage'] = False\n    \n    if chunk.get('has_branches'):\n        payload['has_branches'] = chunk['has_branches']\n    \n    # Check if original ai_interpretations exists and is non-empty\n    # This ensures the flag is set correctly for all platforms\n    has_interpretations = bool(chunk.get('ai_interpretations'))\n    \n    # Add extracted interpretation fields to payload if they exist\n    if chunk.get('about_user'):\n        payload['about_user'] = chunk['about_user']\n    if chunk.get('about_model'):\n        payload['about_model'] = chunk['about_model']\n    \n    payload['has_interpretations'] = has_interpretations\n    \n    # Convert sparse weights to Qdrant format\n    if sparse_weights:\n        indices = list(sparse_weights.keys())\n        values = list(sparse_weights.values())\n        sparse_vector = SparseVector(indices=indices, values=values)\n    else:\n        sparse_vector = SparseVector(indices=[], values=[])\n    \n    point = PointStruct(\n        id=idx,\n        vector={\n            \"dense\": dense_emb.tolist(),\n            \"sparse\": sparse_vector\n        },\n        payload=payload\n    )\n    points.append(point)\n    \n    # Upload in batches of 100\n    if len(points) >= 100 or idx == len(chunks) - 1:\n        client.upsert(\n            collection_name=COLLECTION_NAME,\n            points=points\n        )\n        points = []\n\nprint(\"‚úÖ Upload complete with hybrid (dense + sparse) vectors!\")\nprint(f\"   Total points uploaded: {len(chunks):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "collection_info = client.get_collection(COLLECTION_NAME)\n\n# Count platforms from uploaded chunks\nplatform_counts = {}\nfor chunk in chunks:\n    platform = chunk.get('platform', 'unknown')\n    platform_counts[platform] = platform_counts.get(platform, 0) + 1\n\nprint(\"=\"*70)\nprint(\"‚úÖ MULTI-PLATFORM HYBRID SEARCH UPLOAD COMPLETE!\")\nprint(\"=\"*70)\nprint(f\"Collection: {COLLECTION_NAME}\")\nprint(f\"Total points: {collection_info.points_count:,}\")\nprint(f\"Vector size: {vector_size}\")\nprint(f\"Embedding mode: {EMBEDDING_MODE}\")\n\nprint(f\"\\nPlatform breakdown:\")\nfor platform, count in sorted(platform_counts.items()):\n    print(f\"  {platform}: {count:,} chunks\")\n\nprint(f\"\\nHybrid Search Enabled:\")\nprint(f\"  ‚úÖ Dense vectors (semantic similarity)\")\nprint(f\"  ‚úÖ Sparse vectors (lexical/keyword matching)\")\n\nprint(f\"\\nüîç Ready for cross-platform hybrid search!\")\nprint(f\"   - ChatGPT conversations\")\nprint(f\"   - Claude conversations\")\nprint(f\"   - Claude Projects (user memory, docs, custom instructions)\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}