{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# WillGPT ‚Üí Qdrant Upload (Google Colab)\n\nGenerate BGE-M3 embeddings and upload to Qdrant using Colab GPU.\n\n**Runtime**: GPU (T4 or better recommended)\n\n## Setup Instructions:\n1. Runtime ‚Üí Change runtime type ‚Üí GPU\n2. Run cells in order\n3. Upload your `processed_conversations.json` when prompted\n4. Update configuration in cell 2 with your Qdrant credentials"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q qdrant-client sentence-transformers tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Qdrant Configuration\nQDRANT_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your API key\nQDRANT_URL = \"YOUR_QDRANT_URL_HERE\"   # Replace with your Qdrant URL\nCOLLECTION_NAME = \"will-gpt\"\n\n# Embedding Configuration\nMODEL_NAME = \"BAAI/bge-m3\"\nEMBEDDING_MODE = \"user_focused\"  # Options: balanced, user_focused, minimal, full\nBATCH_SIZE = 32  # GPU can handle larger batches\n\nprint(\"‚úÖ Configuration loaded\")\nprint(f\"   Embedding mode: {EMBEDDING_MODE}\")\nprint(f\"   Batch size: {BATCH_SIZE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload processed_conversations.json\n",
    "\n",
    "**Upload your `data/processed_conversations.json` file using the file upload button on the left sidebar.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "print(\"Upload processed_conversations.json:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify file\n",
    "if 'processed_conversations.json' in uploaded:\n",
    "    print(\"‚úÖ File uploaded successfully\")\n",
    "else:\n",
    "    print(\"‚ùå Please upload processed_conversations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm.auto import tqdm\n\n# Load conversations\nprint(\"Loading conversations...\")\nwith open('processed_conversations.json', 'r') as f:\n    data = json.load(f)\n\n# Handle optimized format with deduplicated interpretations\ninterpretations_store = data.get('interpretations', {})\nchunks = data['chunks']\n\n# Restore interpretations from references\nfor chunk in chunks:\n    if 'ai_interpretation_ref' in chunk:\n        interp_ref = chunk['ai_interpretation_ref']\n        interp = interpretations_store.get(interp_ref, {})\n        \n        # Extract about_user and about_model for easier access\n        user_context = interp.get('user_context_message_data', {})\n        chunk['about_user'] = user_context.get('about_user_message', '')\n        chunk['about_model'] = user_context.get('about_model_message', '')\n\nprint(f\"‚úÖ Loaded {len(chunks)} chunks\")\nprint(f\"   Unique interpretations: {len(interpretations_store)}\")\n\n# Check GPU\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"\\nDevice: {device}\")\nif device == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\n# Load BGE-M3\nprint(f\"\\nLoading {MODEL_NAME}...\")\nmodel = SentenceTransformer(MODEL_NAME, device=device)\nvector_size = model.get_sentence_embedding_dimension()\nprint(f\"‚úÖ Model loaded (vector size: {vector_size})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Embedding Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_embedding_text(chunk, mode=\"balanced\", max_assistant_chars=3000):\n",
    "    \"\"\"Generate embedding text from chunk\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Topic\n",
    "    if chunk.get('conversation_title') and chunk['conversation_title'] != \"Untitled\":\n",
    "        parts.append(f\"[TOPIC: {chunk['conversation_title']}]\")\n",
    "    \n",
    "    # User message\n",
    "    if chunk.get('user_message'):\n",
    "        if mode == \"minimal\":\n",
    "            return chunk['user_message']\n",
    "        parts.append(chunk['user_message'])\n",
    "    \n",
    "    # Assistant\n",
    "    if chunk.get('assistant_message') and mode in [\"balanced\", \"full\"]:\n",
    "        assistant = chunk['assistant_message']\n",
    "        if mode == \"balanced\" and len(assistant) > max_assistant_chars:\n",
    "            half = max_assistant_chars // 2\n",
    "            assistant = assistant[:half] + \"\\n[...]\\n\" + assistant[-half:]\n",
    "        parts.append(f\"[RESPONSE] {assistant}\")\n",
    "    \n",
    "    # AI interpretations\n",
    "    if chunk.get('about_user'):\n",
    "        parts.append(f\"[AI_UNDERSTANDING] {chunk['about_user']}\")\n",
    "    if chunk.get('about_model'):\n",
    "        parts.append(f\"[AI_NOTES] {chunk['about_model']}\")\n",
    "    \n",
    "    return '\\n\\n'.join(parts)\n",
    "\n",
    "# Generate texts\n",
    "print(\"Generating embedding texts...\")\n",
    "embedding_texts = [to_embedding_text(chunk, mode=EMBEDDING_MODE) for chunk in tqdm(chunks)]\n",
    "print(f\"‚úÖ Generated {len(embedding_texts)} texts\")\n",
    "print(f\"\\nAvg length: {sum(len(t) for t in embedding_texts) / len(embedding_texts):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating embeddings (batch size: {BATCH_SIZE})...\")\n",
    "embeddings = model.encode(\n",
    "    embedding_texts,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Convert to CPU numpy for upload\n",
    "embeddings = embeddings.cpu().numpy()\n",
    "print(f\"‚úÖ Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Connect to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "print(\"Connecting to Qdrant...\")\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    prefer_grpc=False,\n",
    ")\n",
    "\n",
    "# Check existing collections\n",
    "collections = client.get_collections()\n",
    "print(f\"‚úÖ Connected!\")\n",
    "print(f\"Existing collections: {[c.name for c in collections.collections]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if collection exists\n",
    "collection_exists = any(c.name == COLLECTION_NAME for c in collections.collections)\n",
    "\n",
    "if collection_exists:\n",
    "    print(f\"‚ö†Ô∏è  Collection '{COLLECTION_NAME}' exists!\")\n",
    "    delete = input(\"Delete and recreate? (yes/no): \")\n",
    "    if delete.lower() == 'yes':\n",
    "        client.delete_collection(COLLECTION_NAME)\n",
    "        print(f\"Deleted existing collection\")\n",
    "    else:\n",
    "        print(\"Keeping existing collection (will update points)\")\n",
    "\n",
    "if not collection_exists or delete.lower() == 'yes':\n",
    "    print(f\"\\nCreating collection '{COLLECTION_NAME}'...\")\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config={\n",
    "            \"dense\": VectorParams(\n",
    "                size=vector_size,\n",
    "                distance=Distance.COSINE,\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    print(f\"‚úÖ Collection created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Upload to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nUploading {len(embeddings)} points to Qdrant...\")\n",
    "\n",
    "# Create points\n",
    "points = []\n",
    "for idx, (chunk, embedding) in enumerate(tqdm(zip(chunks, embeddings), total=len(chunks))):\n",
    "    payload = {\n",
    "        \"conversation_id\": chunk.get('conversation_id'),\n",
    "        \"platform\": chunk.get('platform'),\n",
    "        \"timestamp\": chunk.get('timestamp'),\n",
    "        \"conversation_title\": chunk.get('conversation_title'),\n",
    "        \"turn_number\": chunk.get('turn_number'),\n",
    "        \"user_message\": chunk.get('user_message'),\n",
    "        \"assistant_message\": chunk.get('assistant_message'),\n",
    "        \"assistant_model\": chunk.get('assistant_model'),\n",
    "        \"has_interpretations\": chunk.get('has_interpretations', False),\n",
    "    }\n",
    "    \n",
    "    # Add interpretations if present\n",
    "    if chunk.get('about_user'):\n",
    "        payload['about_user'] = chunk['about_user']\n",
    "    if chunk.get('about_model'):\n",
    "        payload['about_model'] = chunk['about_model']\n",
    "    \n",
    "    point = PointStruct(\n",
    "        id=idx,\n",
    "        vector={\"dense\": embedding.tolist()},\n",
    "        payload=payload\n",
    "    )\n",
    "    points.append(point)\n",
    "    \n",
    "    # Upload in batches\n",
    "    if len(points) >= 100 or idx == len(chunks) - 1:\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=points\n",
    "        )\n",
    "        points = []\n",
    "\n",
    "print(\"‚úÖ Upload complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_info = client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ UPLOAD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Collection: {COLLECTION_NAME}\")\n",
    "print(f\"Total points: {collection_info.points_count}\")\n",
    "print(f\"Vector size: {vector_size}\")\n",
    "print(f\"Embedding mode: {EMBEDDING_MODE}\")\n",
    "print(f\"\\nüîç Ready for hybrid search!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}